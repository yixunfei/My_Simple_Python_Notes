
# MySQL

## 原生`pymysql`


### 基础流程

#### 安装

使用 `pymysql` 模块，这是Python的一个MySQL驱动程序。如果还没有安装pymysql，可以通过`pip`命令来安装：

```Bash
   pip install pymysql
```

#### `connect()`创建连接 

使用 `pymysql.connect()` 函数创建一个数据库连接。你需要提供主机名、端口、用户名、密码、数据库名和字符集等参数。

#### `cursor()`创建游标

游标允许你执行SQL语句并获取结果。通过连接对象调用 `cursor()` 方法创建游标

#### `execute()`执行SQL语句

使用游标对象的`execute()`方法执行SQL语句。例如，你可以执行查询、插入、更新或删除操作

#### 获取结果

对于查询操作，可以使用`fetchone()`,` fetchall()`, 或者 `fetchmany(size)` 方法获取结果。

#### 提交事务

如果执行的是修改数据库的操作，如插入、更新或删除，需要调用连接对象的`commit()`方法来提交事务

#### 关闭游标和连接 完成

数据库操作后，记得 `cursor.close()` 关闭游标和 `conn.close()` 连接以释放资源。

### 示例

```python
import pymysql

# 创建连接
conn = pymysql.connect(
    host='localhost',
    port=3306,
    user='root',
    password='your_password',
    database='your_database',
    charset='utf8'
)

# 创建游标
cursor = conn.cursor()

# 执行SQL语句
sql = "SELECT * FROM your_table"
cursor.execute(sql)

# 获取查询结果
results = cursor.fetchall()
for row in results:
    print(row)

# 关闭游标
cursor.close()

# 关闭连接
conn.close()
```

**如果执行insert**：

```python
# 执行插入操作
insert_sql = "INSERT INTO your_table (column1, column2) VALUES (%s, %s)"
values = ('value1', 'value2')
cursor.execute(insert_sql, values)
conn.commit()
```

### `pymysql` 事务

在 `pymysql` 中，**事务默认是关闭的**，这意味着每次执行 SQL 语句后都会自动提交。为了启用事务，需要显式地开始事务，然后根据需要提交或回滚事务。

`pymysql` 的事务管理依赖于 MySQL 数据库的事务特性，需要确保你的数据库引擎（如 InnoDB）支持事务。

使用`cursor.execute("START TRANSACTION")`显示的向数据库请求开启事务，`connection.commit()` 提交事务,`connection.rollback()` 回滚事务。

```python
import pymysql

# 创建数据库连接
connection = pymysql.connect(host='localhost',
                             user='your_username',
                             password='your_password',
                             database='your_database',
                             charset='utf8mb4',
                             cursorclass=pymysql.cursors.DictCursor)

try:
    with connection.cursor() as cursor:
        # 开始事务
        cursor.execute("START TRANSACTION")

        # 执行一系列的 SQL 语句
        sql1 = "INSERT INTO your_table (column1, column2) VALUES (%s, %s)"
        data1 = ('value1', 'value2')
        cursor.execute(sql1, data1)

        sql2 = "UPDATE another_table SET column3 = %s WHERE column4 = %s"
        data2 = ('new_value', 'some_condition')
        cursor.execute(sql2, data2)

        # 其他 SQL 语句...

        # 提交事务
        connection.commit()
except Exception as e:
    # 如果在事务中发生错误，回滚事务
    connection.rollback()
    print(f"An error occurred: {e}")
finally:
    # 关闭数据库连接
    connection.close()
```


---
## `SQLAlchemy`框架(ORM)

### 安装

可以通过 `pip` 安装 `SQLAlchemy` ：

```Bash
pip install sqlalchemy
```

### 基本概念

- **元数据（Metadata）**：定义了数据库模式的信息。
- **表（Table）**：数据库中的表格，可以通过Table类定义。
- **映射（Mapping）**：将Python类映射到数据库表。
- **会话（Session）**：用于与数据库进行交互的上下文。


### 基础流程

#### 创建数据库引擎
```python
from sqlalchemy import create_engine

engine = create_engine('mysql://user:password@localhost/dbname')
```

这里的URL格式为`dialect+driver://username:password@host:port/database`。

#### 定义表结构

```python
from sqlalchemy import Column, Integer, String, MetaData, Table

metadata = MetaData()

users = Table('users', metadata,
    Column('id', Integer, primary_key=True),
    Column('name', String(50)),
    Column('age', Integer)
)
```

#### 映射Python类到表

```python
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import Column, Integer, String

Base = declarative_base()

class User(Base):
    __tablename__ = 'users'

    id = Column(Integer, primary_key=True)
    name = Column(String(50))
    age = Column(Integer)
```

#### 创建表

```python
Base.metadata.create_all(engine)
```

#### 插入数据

```python
from sqlalchemy.orm import sessionmaker

Session = sessionmaker(bind=engine)
session = Session()

new_user = User(name='zhangsan', age=30)
session.add(new_user)
session.commit()
```

#### 查询数据

```python
users = session.query(User).filter(User.name == 'zhangsan').all()
for user in users:
    print(user.name, user.age)
```

#### 更新数据

```python
user = session.query(User).filter(User.name == 'zhangsan').first()
user.age = 31
session.commit()
```

#### 删除数据

```python
user = session.query(User).filter(User.name == 'zhangsan').first()
session.delete(user)
session.commit()
```

#### 关闭会话

```python
session.close()
```


### `SQLAlchemy` 事务

#### 使用 `with` 语句自动管理

使用 `with` 语句可以自动管理事务，这会确保在代码块结束时，如果没有任何异常，事务会被提交；如果有异常，则事务会被回滚

```python
from sqlalchemy.orm import Session

# 假设 session 是一个已创建的 Session 对象
with Session(engine) as session:
    with session.begin():
        # 在这里执行数据库操作
        user = User(name="zhangsan")
        session.add(user)
        # 更多操作...
```

#### 使用 session 手动管理

`session.begin()` 显示的开始一个事务,`session.commit()` 提交事务，`session.rollback()` 回滚事务

```python
session = Session(engine)
try:
    transaction = session.begin()
    # 执行数据库操作
    user = User(name="zhangsan")
    session.add(user)
    # 更多操作...
    transaction.commit()
except Exception:
    transaction.rollback()
    raise
finally:
    session.close()
```

```python
session = Session(engine)
try:
    # 执行数据库操作
    user = User(name="aaaaa")
    session.add(user)
    # 更多操作...
    session.commit()
except Exception:
    session.rollback()
    raise
finally:
    session.close()
```
#### `session.flush()`

`flush()` 方法是在事务管理内与数据库发生交互，它会将当前会话中的所有变更同步到数据库，但不会提交事务

```python
session = Session(engine)
try:
    user = User(name="bbbbbb")
    session.add(user)
    session.flush()  # 此时用户将被添加到数据库，但事务未提交
    # 可以在此处获取自动生成的ID等信息
    session.commit()
except Exception:
    session.rollback()
    raise
finally:
    session.close()
```

**在处理事务时，确保正确地使用 `try...except...finally` 结构来捕获任何可能发生的异常，并适当地回滚事务，以防止数据库处于不一致的状态。**

---

# MongoDB

## 原生 `pymongo`

`pymongo`是python下mongodb的一个驱动(api大致与mongo的api方法相同)

### 安装

```Bash
pip install pymongo
```

### 连接MongoDB


使用`pymongo.MongoClient`类来创建与MongoDB的连接。

```python
from pymongo import MongoClient

client = MongoClient('mongodb://localhost:27017/')
```
或者，使用带有认证的连接字符串：
```python
client = MongoClient('mongodb://username:password@host:port/dbname')
```

### 选择数据库和集合

```python
db = client['mydatabase']
collection = db['mycollection']
```

#### 插入文档

使用 `insert_one()` 或 `insert_many()` 方法,插入文档到集合中

```python
result = collection.insert_one({"name": "zhangsan", "age": 30})
print(result.inserted_id)  # 输出新文档的ID

documents = [
    {"name": "zhangsan", "age": 25},
    {"name": "lisi", "age": 35}
]
result = collection.insert_many(documents)
print(result.inserted_ids)  # 输出新文档的ID列表
```

#### 查询文档

使用 `find_one()` 和 `find()` 方法查询文档

```python
# 查询单个文档
document = collection.find_one({"name": "zhangsan"})
print(document)

# 查询多个文档
documents = collection.find({"age": {"$gt": 30}})
for doc in documents:
    print(doc)
```

#### 更新文档

使用 `update_one()` 或 `update_many()` 方法来更新文档。

```python
result = collection.update_one({"name": "zhangsan"}, {"$set": {"age": 31}})
print(result.matched_count, result.modified_count)

result = collection.update_many({"age": {"$gt": 30}}, {"$inc": {"age": 1}})
print(result.matched_count, result.modified_count)
```

#### 删除文档

使用 `delete_one()` 或 `delete_many()` 方法来删除文档。

```python
result = collection.delete_one({"name": "zhangsan"})
print(result.deleted_count)

result = collection.delete_many({"age": {"$gt": 30}})
print(result.deleted_count)
```

#### 关闭连接

```python
client.close()
```


### `pymongo` 事务


- `start_session()` 开启事务
- `session.commit_transaction()`提交事务
- `session.abort_transaction()` 回滚事务

使用 `start_session()` 方法来创建一个新的会话，然后在该会话中开始事务。

```python
session = client.start_session()
session.start_transaction()
```

提交或回滚

```python
try:
    # 继续执行操作...
    session.commit_transaction()
except Exception as e:
    session.abort_transaction()
    print(e)
```


**示例：**

```python
from pymongo import MongoClient

client = MongoClient('mongodb://localhost:27017/', replicaSet='myReplicaSet')
db = client['mydatabase']

session = client.start_session()
session.start_transaction()

try:
    db.collection1.update_one({'_id': 1}, {'$set': {'field': 'value'}}, session=session)
    db.collection2.update_one({'_id': 1}, {'$set': {'field': 'value'}}, session=session)
    
    session.commit_transaction()
except Exception as e:
    session.abort_transaction()
    print(e)
finally:
    session.end_session()
```

### `read_preference` 指定查询节点

`read_preference` 决定了读取操作（如查询）应该从 MongoDB 集群中的哪些成员执行。这对于提高读取性能和容错性至关重要，尤其是在使用副本集或分片集群的情况下

`read_preference` 可以设置为以下几个选项之一：

- **PRIMARY (默认)**: 所有读取操作都将从主节点（primary）执行。这是默认的读偏好，保证了读取操作的一致性和最新性。
- **PRIMARY_PREFERRED**: 首选从主节点读取，但如果主节点不可用，则可以从从节点（secondary）读取。
- **SECONDARY**: 所有读取操作都将从从节点执行。这可以提高读取性能，因为从节点可以并行处理读取请求，但数据可能不是最新的。
- **SECONDARY_PREFERRED**: 首选从从节点读取，但如果所有从节点均不可用，则可以退回到主节点读取。
- **NEAREST**: 从最近的节点读取，无论是主节点还是从节点。这可以最小化延迟，但数据一致性可能不是最强的。

```python
from pymongo import MongoClient, ReadPreference

client = MongoClient('mongodb://localhost:27017/', read_preference=ReadPreference.SECONDARY_PREFERRED)
db = client['mydatabase']
```

```python
from mongoengine import connect, ReadPreference

# 连接到 MongoDB
connect('mydatabase', host='mongodb://localhost:27017/', read_preference=ReadPreference.SECONDARY_PREFERRED)

# 查询时使用读偏好
user = User.objects(name='zhangsan').first(read_preference=ReadPreference.SECONDARY_PREFERRED)
```



---

## `MongoEngine`(ODM) 框架

`MongoEngine`是一个面向Python的Object-Document Mapping (ODM)框架，用于与MongoDB数据库进行交互。它提供了一种**声明式的模型定义**方式，使得在Python中操作MongoDB文档就像操作Python对象一样简单。

### 安装

```python
pip install mongoengine
```

### 连接到MongoDB

```python
from mongoengine import connect

connect(db='mydatabase', host='localhost', port=27017)
```

### 定义模型

可以像定义Python类那样定义MongoDB的文档模型(继承`Document`)。每个类代表一个集合，**类的属性代表集合中的字段**。

```python
from mongoengine import Document, StringField, IntField

class User(Document):
    meta = {'collection': 'users'}
    name = StringField(required=True, max_length=100)
    age = IntField(required=True)
```

### 插入文档

```python
user = User(name='zhangsan', age=30)
user.save()
```

### 查询文档

查询MongoDB中的文档可以使用模型类的`.objects`属性，它返回一个`QuerySet`对象，可以用来过滤和操作数据。

```python
# 查询所有文档
users = User.objects()

# 根据条件查询
user = User.objects(name='zhangsan').first()

# 复杂查询
users = User.objects(age__gt=25, name__startswith='z')
```

### 更新文档

更新MongoDB中的文档可以通过加载现有文档，修改其属性，然后调用`.save()`方法来完成，或者使用`.update()`方法。

```python
user = User.objects(name='zhangsan').first()
user.age = 31
user.save()

# 或者使用 update 方法
User.objects(name='zhangsan').update_one(inc__age=1)
```

### 删除文档

删除MongoDB中的文档可以通过加载现有文档，然后调用`.delete()`方法来完成。

```python
user = User.objects(name='zhangsan').first()
user.delete()

# 或者使用 delete 方法
User.objects(name='zhangsan').delete()
```

### 关闭连接

```python
from mongoengine import disconnect

disconnect()
```

### MongoEngine 事务

使用 with 语句和 transaction.atomic() 上下文管理器来开始和结束事务。
```python
from mongoengine import Document, StringField, IntField, transaction

class User(Document):
    name = StringField(required=True)
    age = IntField(required=True)

class Post(Document):
    title = StringField(required=True)
    author = StringField(required=True)

with transaction.atomic():
    user = User(name='zhangsan', age=30).save()
    post = Post(title='First Post', author=user.name).save()
```

如果在事务上下文中发生任何错误异常，**事务将自动回滚**，以保持数据一致性

```python
try:
    with transaction.atomic():
        user = User(name='zhangsan', age=25).save()
        # 假设这里有一个错误，例如 post 的 author 字段不存在
        post = Post(title='Second Post', author='NonExistentUser').save()
except Exception as e:
    print(f"Transaction failed: {e}")
```


# Redis

## 原生`redis-py`

`redis-py` 提供了两种主要的类来操作Redis：`StrictRedis` 和 `Redis`。`StrictRedis` 严格遵循Redis官方命令的命名规范，而Redis类则是为了向后兼容旧版本的`redis-py`

### 安装

```Bash
pip install redis
```

### 连接

```python
import redis

r = redis.StrictRedis(host='localhost', port=6379, db=0)
```

### 基本操作

#### 设置键值

```python
r.set('mykey', 'Hello, World!')
```

#### 获取键值

```python
value = r.get('mykey')
print(value.decode())  # 输出: Hello, World!
```

#### list操作

```python
r.rpush('mylist', 'item1', 'item2', 'item3')
items = r.lrange('mylist', 0, -1)
for item in items:
    print(item.decode())
```

#### hash操作

```python
r.hset('myhash', 'field1', 'value1')
r.hset('myhash', 'field2', 'value2')
fields = r.hgetall('myhash')
for field, value in fields.items():
    print(field.decode(), value.decode())
```


#### set操作

```python
r.sadd('myset', 'member1', 'member2', 'member3')
members = r.smembers('myset')
for member in members:
    print(member.decode())
```

#### zset操作

```python
r.zadd('mysortedset', {'member1': 1.0, 'member2': 2.0, 'member3': 3.0})
scores = r.zrange('mysortedset', 0, -1, withscores=True)
for member, score in scores:
    print(member.decode(), score)
```

#### Pub/Sub 操作

#### Sub

使用 `PubSub` 类订阅一个或多个频道,使用`pubsub.subscribe`订阅，`listen()` 监听。

```python
import redis

r = redis.Redis()

pubsub = r.pubsub()
pubsub.subscribe('my-channel')

for message in pubsub.listen():
    if message['type'] == 'message':
        print(f"Received message: {message['data'].decode()}")

```

#### Pub

使用 `publish` 向一个进行推送

```python
r.publish('my-channel', 'Hello, subscribers!')
```


#### `ConnectionPool` 连接池

`redis-py`提供了 `ConnectionPool` 类来管理连接。

**可以通过 `max_connections` 参数来设置最大连接数。**

```python
from redis import ConnectionPool, Redis

pool = ConnectionPool(host='localhost', port=6379, db=0, max_connections=10)
r = Redis(connection_pool=pool)
```


#### 关闭连接

`redis-py`的连接是可重用的，通常情况下你不需要显式地关闭连接。连接会在下次使用时被自动重用。如果希望显式地关闭连接池并释放**所有**连接，可以调用 `disconnect()` 方法
```python
pool.disconnect()
```

#### 异常处理

```python
try:
    value = r.get('nonexistent_key')
except redis.exceptions.ConnectionError as e:
    print("Connection error:", e)
except redis.exceptions.ResponseError as e:
    print("Response error:", e)
```
### `hiredis`

`hiredis` 是一个用 C 语言编写的高性能 Redis 客户端库，它专注于快速解析 Redis 协议。`hiredis` 的设计目标是提供低延迟和高吞吐量

**`hiredis` 特性**

- **高性能**：hiredis 通过高效的协议解析和异步 I/O 支持，能够处理大量的并发连接和请求。
- **低延迟**：由于其高效的协议解析，hiredis 能够显著降低请求的处理时间，非常适合对延迟敏感的应用场景。
- **异步支持**：hiredis 支持非阻塞 I/O 模型，可以与其他事件驱动的库（如 libev 或 libevent）结合使用，实现真正的异步 Redis 客户端。
- **简单易用**：尽管性能强大，hiredis 的 API 设计简洁，易于理解和使用。

在 Python 中，`redis-py` 客户端可以配置为使用 `hiredis` 作为底层的 Redis 协议解析器，以提高性能。
##### 安装

```Bash
pip install hiredis
```

##### 使用

```python
import redis

# 创建 Redis 连接，使用 hiredis 解析器
r = redis.Redis(decode_responses=True, parser_class=redis.HiredisParser)

# 执行 Redis 命令
r.set('mykey', 'Hello, world!')
print(r.get('mykey'))
```

---

## `redis-py-cluster` 集群

`redis-py-cluster` 是一个用于与 Redis Cluster 环境进行交互的 Python 库。`redis-py-cluster` 基于 `redis-py` 构建，增加了对 Redis Cluster 特有的功能支持，如:

- **自动槽位发现**：Redis Cluster 将整个键空间划分为 16384 个槽，每个槽被分配给集群中的一个节点。`redis-py-cluster` *能够自动发现这些槽的分布，并将请求路由到正确的节点*。
- **故障转移和重定向**：当集群中的某个节点失效时，`redis-py-cluster` *可以自动检测并重新路由请求到新的主节点或从节点*，以保持应用程序的连续性。
- **读写分离**：`redis-py-cluster` 支持*从节点的读取操作*，可以提高读取性能和容错性。
- **透明的集群操作**：对于开发者来说，使用 `redis-py-cluster` 与使用普通的 `redis-py` 类似，大部分 Redis 命令可以直接使用，而*不需要关心底层的集群细节*
### 安装

使用`redis-py-cluster` 需要前置安装`redis-py`,同时也可以使用`hiredis`提高性能

```Bash
pip install redis 
pip install hiredis
pip install redis-py-cluster
```

### 示例

```python
from rediscluster import RedisCluster

startup_nodes = [
    {"host": "127.0.0.1", "port": "7000"},
    {"host": "127.0.0.1", "port": "7001"},
    {"host": "127.0.0.1", "port": "7002"}
]

rc = RedisCluster(startup_nodes=startup_nodes, decode_responses=True)

# 执行 Redis 命令
rc.set('mykey', 'Hello, world!')
print(rc.get('mykey'))  # 输出: Hello, world!
```


# Elasticsearch

## `elasticsearch` 库
`elasticsearch`是由Elastic官方提供的Python客户端，用于与Elasticsearch集群进行交互。

### 安装

```Bash
pip install elasticsearch
```

### 连接

```python
from elasticsearch import Elasticsearch

# es = Elasticsearch() 本机
es = Elasticsearch(['http://192.168.1.100:9200'])
```

### 索引操作

```python
# 创建索引
es.indices.create(index='my_index', ignore=400)

# 删除索引
es.indices.delete(index='my_index', ignore=[400, 404])

# 检查索引是否存在
es.indices.exists(index='my_index')
```

### 文档操作

```python
# 添加文档
doc = {
    'author': 'test',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'timestamp': datetime.now(),
}
res = es.index(index="my_index", id=1, body=doc)
print(res['result'])

# 获取文档
res = es.get(index="my_index", id=1)
print(res['_source'])

# 更新文档
doc = {
    'doc': {
        'text': 'Elasticsearch: cool. bonsai cool. updated!'
    }
}
res = es.update(index="my_index", id=1, body=doc)
print(res['result'])

# 删除文档
res = es.delete(index="my_index", id=1)
print(res['result'])
```

### 查询文档

直接使用DSL查询

```python
res = es.search(index="my_index", body={"query": {"match_all": {}}})
print("Got %d Hits:" % res['hits']['total']['value'])
for hit in res['hits']['hits']:
    print("%(timestamp)s %(author)s: %(text)s" % hit["_source"])
```

### 批量操作

使用 `helpers` 模块进行批量操作，例如批量索引文档：

```python
from elasticsearch.helpers import bulk

actions = [
    {
        "_index": "my_index",
        "_id": i,
        "_source": {
            "any": f"document {i}",
            "timestamp": datetime.now(),
        },
    }
    for i in range(1, 5001)
]

success, _ = bulk(es, actions, index="my_index")
print('Performed %d actions' % success)
```

### 异常处理

```python
from elasticsearch import TransportError

try:
    es.info()
except TransportError as e:
    print("Transport error:", e)

```

### 复杂查询

Elasticsearch 提供了丰富的查询 DSL，允许你执行复杂的搜索和聚合。例如 term 查询、range 查询、bool 查询等来精确匹配

```python
# 使用 term 查询
res = es.search(index="my_index", body={
    "query": {
        "term": {
            "author": "test"
        }
    }
})

# 使用 range 查询
res = es.search(index="my_index", body={
    "query": {
        "range": {
            "timestamp": {
                "gte": "1970-01-01T00:00:00",
                "lte": "1970-01-31T23:59:59"
            }
        }
    }
})

# 使用 bool 查询组合多个条件
res = es.search(index="my_index", body={
    "query": {
        "bool": {
            "must": [
                {"term": {"author": "test"}},
                {"range": {"timestamp": {"gte": "1970-01-01T00:00:00"}}}
            ]
        }
    }
})
```

### 聚合分析

```python
# 计算所有文档中 timestamp 字段的平均值
res = es.search(index="my_index", body={
    "aggs": {
        "avg_timestamp": {
            "avg": {
                "field": "timestamp"
            }
        }
    }
})

# 分组统计 author 字段的不同值
res = es.search(index="my_index", body={
    "aggs": {
        "author_stats": {
            "terms": {
                "field": "author.keyword"
            }
        }
    }
})
```


### 分析器和映射

```python
# 创建索引并定义映射
es.indices.create(index='my_index', body={
    "settings": {
        "analysis": {
            "analyzer": {
                "my_analyzer": {
                    "type": "standard",
                    "stopwords": "_english_"
                }
            }
        }
    },
    "mappings": {
        "properties": {
            "text": {
                "type": "text",
                "analyzer": "my_analyzer"
            }
        }
    }
})
```

### 安全性和身份验证


在生产环境中，可能需要配置 Elasticsearch 的安全性和身份验证。`elasticsearch` 客户端支持 HTTP 基本身份验证和 TLS/SSL：

```python
from elasticsearch import Elasticsearch

es = Elasticsearch(
    ['http://192.168.1.100:9200'],
    http_auth=('username', 'password'),
    scheme="https",
    verify_certs=True,
    ca_certs="/path/to/ca/certs.pem"
)
```

### 监控和日志

```python
# 获取集群健康状态
res = es.cluster.health()

# 获取节点统计信息
res = es.nodes.stats(metric="os,jvm")
```


### 机器学习(es的Machine Learning模块)

#### 安装

需要额外安装elasticsearch-ml库

```Bash
pip install elasticsearch elasticsearch-ml
```

#### 示例(异常检测)

```python
from datetime import datetime
from elasticsearch import Elasticsearch
from elasticsearch.client import MlClient

es = Elasticsearch()
ml = MlClient(es)

# 定义异常检测作业
job_config = {
    "description": "Sample anomaly detection job",
    "analysis_config": {
        "bucket_span": "1h",
        "detectors": [
            {
                "detector_description": "CPU usage",
                "function": "count",
                "by_field_name": "host"
            }
        ]
    },
    "data_description": {
        "time_field": "@timestamp",
        "time_format": "epoch_ms"
    }
}

# 创建异常检测作业
job_id = "sample_job"
response = ml.put_job(job_id=job_id, body=job_config)
print(response)

# 启动作业
response = ml.start_job(job_id=job_id)
print(response)

# 检查作业状态
response = ml.get_job(job_id=job_id)
print(response)

# 停止作业
response = ml.stop_job(job_id=job_id)
print(response)

# 删除作业
response = ml.delete_job(job_id=job_id)
print(response)
```


### 跨集群搜索

#### 配置远程集群

```python
es = Elasticsearch()

# 动态注册远程集群
remote_cluster_config = {
    "seeds": ["192.168.160.245:9300"],
    "skip_unavailable": True
}
response = es.cluster.put_settings(
    body={
        "transient": {
            "cluster.remote.my_remote_cluster": remote_cluster_config
        }
    }
)
print(response)
```

#### 执行跨集群搜索

```python
# 执行跨集群搜索
response = es.search(
    index="remote-index-*",
    body={
        "query": {
            "remote": {
                "cluster": "my_remote_cluster",
                "query": {
                    "match_all": {}
                }
            }
        }
    }
)
print(response)
```

---

## `elasticsearch-dsl` (DSL 映射)框架

`elasticsearch-dsl`是一个建立在 `elasticsearch` Python客户端之上的高级库，它提供了一种更加Pythonic的方式来与Elasticsearch交互。该库的主要优势在于它允许你使用Python的**对象和方法来构建复杂的搜索请求，而不是直接编写JSON查询**。这使得代码更易于阅读和维护。

- **动态映射**：**自动检测文档结构并映射**到相应的字段类型。
- **聚合**：使用Python方法构建聚合查询。
- **更新文档**：使用`update()`方法更新文档的部分内容。
- **高亮显示**：在搜索结果中高亮显示匹配的文本。
- **脚本**：支持使用Elasticsearch的脚本来修改文档或计算值。

### 安装

使用`elasticsearch-dsl`需要前置安装`elasticsearch`

```Bash
pip install elasticsearch elasticsearch-dsl
```

### 定义文档类型

```python
from elasticsearch_dsl import Document, Text, Date

class BlogPost(Document):
    title = Text(analyzer='snowball')
    body = Text(analyzer='snowball')
    published_from = Date()

    class Index:
        name = 'blogposts'
```

### 索引文档

使用 `save()` 方法来索引文档

```python
post = BlogPost(
    meta={'id': '1'},
    title='Hello World!',
    body='This is my first blog post.',
    published_from='2015-01-01'
)
post.save()
```

### 搜索文档

使用 `search()` 方法来执行搜索

```python
s = BlogPost.search()
response = s.execute()

for hit in response:
    print(hit.title)
```

### 构建复杂查询

`elasticsearch-dsl`允许你以一种非常直观的方式构建复杂的查询

```python
s = BlogPost.search()
s = s.query("match", title="Hello") \
    .filter("range", published_from={"gte": "2015-01-01"}) \
    .sort({"published_from": {"order": "desc"}})

response = s.execute()
```


## 其他es框架

- `elasticsearch-async`
	异步版本的elasticsearch库，基于 `aiohttp`，适用于需要异步IO的环境，如 `asyncio`
	- 使用场景：在需要非阻塞IO的异步应用中使用，如Web服务或微服务架构。
- `elasticsearch-py`
	实际上 `elasticsearch` 和 `elasticsearch-async` 都是 `elasticsearch-py` 项目的一部分，这里提到是为了澄清项目结构。
	-  使用场景：同 `elasticsearch` 和 `elasticsearch-async`。
- `elasticsearch-helper`
	提供了一些辅助工具，如批量导入、导出和数据清洗，简化了数据管理和迁移任务。
	- 使用场景：数据迁移、批量操作和数据清洗。
- `zapi_elastic_search`
	一个第三方库，提供了一些简化的接口来操作Elasticsearch，适合快速原型设计和小规模应用。
	- 使用场景：快速开发和小型项目。
- `es-pandas`
	这个库提供了将Elasticsearch数据转换为Pandas DataFrame的工具，便于数据分析。
	-  使用场景：数据分析和数据科学项目，特别是需要将Elasticsearch数据与Pandas结合使用的场景。


# Clickhouse

## 原生`clickhouse-driver`

`clickhouse-driver` 是一个纯Python的ClickHouse驱动程序，支持异步和同步操作。支持同步或异步的方式与ClickHouse数据库进行交互。

### 安装

```Bash
pip install clickhouse-driver
```

### 连接

```python
from clickhouse_driver import Client

client = Client('localhost')
```

### 执行SQL语句

```python
# 创建表
client.execute('CREATE TABLE test_table (x Int32) ENGINE = Memory')

# 插入数据
client.execute('INSERT INTO test_table (x) VALUES', [(1,), (2,), (3,)])

# 查询数据
result = client.execute('SELECT * FROM test_table')
print(result)

# 删除表
client.execute('DROP TABLE test_table')
```

### 异步操作

#### 异步客户端

```python
from clickhouse_driver import AsyncClient

async def main():
    async with AsyncClient('localhost') as client:
        await client.execute('CREATE TABLE test_table (x Int32) ENGINE = Memory')
        await client.execute('INSERT INTO test_table (x) VALUES', [(1,), (2,), (3,)])
        result = await client.execute('SELECT * FROM test_table')
        print(result)
        await client.execute('DROP TABLE test_table')

# 运行异步函数
import asyncio
asyncio.run(main())
```

### 自定义设置

```python
client = Client('localhost', settings={'use_numpy': True})
```

 **数据压缩传输**
 
ClickHouse支持在传输层对数据进行压缩，以减少网络带宽消耗。clickhouse-driver允许你通过设置compression参数来启用数据压缩。

```python
from clickhouse_driver import Client

client = Client('localhost', compression=True)

# 执行查询
result = client.execute('SELECT * FROM large_table')
```

```python
# 设置自定义参数
client = Client('localhost', settings={'max_block_size': 100000})

# 执行查询
result = client.execute('SELECT * FROM large_table')
```
### 批量操作

批量插入数据可以显著提高写入速度，尤其是在处理大量数据时。clickhouse-driver支持通过execute方法的data参数批量插入数据

```python
# 准备数据
data = [(i,) for i in range(100000)]

# 批量插入
client.execute('INSERT INTO test_table (x) VALUES', data)
```

### 数据立方体

数据立方体（Data Cube）是数据仓库中用于多维数据分析（OLAP）的核心概念。它提供了一种组织和查看多维数据集的方式，使用户能够快速执行聚合查询和数据分析。数据立方体的概念来源于多维数据模型，它将数据组织成一个多维网格，每个维度代表数据的一个方面或分类。

#### 数据立方体的基本构成

- **维度（Dimensions）**：维度是观察数据的角度或分类，例如时间、地点、产品类别等。每个维度都有层次结构，允许用户进行不同级别的细节查看。
- **度量（Measures）**：度量是数据立方体中实际存储的数值属性，如销售额、数量、利润等，它们通常与维度组合在一起形成数据立方体的单元格。
- **单元格（Cells）**：数据立方体的每个交叉点称为单元格，它表示一组特定维度值下的度量值。

#### 数据立方体的类型

- **完全立方体（Full Cube）**：包含所有可能的维度组合的立方体，提供最完整的数据视图，但可能占用大量存储空间。
- **冰山立方体（Iceberg Cube）**：只存储预聚合的数据，通常用于处理稀疏数据集，以节省存储空间。
- **闭立方体（Closed Cube）**：存储所有维度组合的最小集合，其中每个维度组合的值都大于或等于其父级维度组合的值。
- **立方体外壳（Cube Shell）**：存储立方体的边缘部分，即最常被查询的维度组合

#### 数据立方体的操作

数据立方体支持多种操作，包括：

- **切片（Slice）**：选择一个维度的特定值，以查看更具体的细节。
- **切块（Dice）**：选择多个维度的特定值，以查看更具体的子集。
- **钻取（Drill-down / Drill-up）**：在维度的层次结构中移动，以查看更详细或更概括的数据。
- **旋转（Pivot）**：改变维度的位置，以不同的视角查看数据。

#### 数据立方体的计算和存储

计算数据立方体时，可以采用多种算法，如星型模式、雪花模式、BUC（Bottom-Up Computation）算法等，以提高查询性能和减少存储需求。

#### 物化视图

在ClickHouse中，物化视图可以被看作是一种实现数据立方体功能的技术手段。通过精心设计的物化视图，可以有效地实现数据立方体的多维分析能力，同时利用ClickHouse的高性能查询特性。例如，可以创建一个物化视图，它聚合了按时间、地区和产品分类的销售数据，类似于一个数据立方体。


##  `clickhouse-sqlalchemy` (ORM)框架

```Bash
pip install sqlalchemy clickhouse-sqlalchemy
```

```python
from sqlalchemy import create_engine, Table, Column, Integer, MetaData

engine = create_engine('clickhouse://localhost:9000/default')

metadata = MetaData()

table = Table(
    'test_table', metadata,
    Column('x', Integer),
)

metadata.create_all(engine)

with engine.connect() as connection:
    connection.execute(table.insert(), [{'x': 1}, {'x': 2}])

    result = connection.execute(table.select()).fetchall()
    print(result)

    table.drop(engine)
```

## `dbapi` (DB-API)

```python
import clickhouse_driver.dbapi

conn = clickhouse_driver.dbapi.connect(host='localhost')

cursor = conn.cursor()

cursor.execute('CREATE TABLE test_table (x Int32) ENGINE = Memory')
cursor.execute('INSERT INTO test_table (x) VALUES (1), (2), (3)')
cursor.execute('SELECT * FROM test_table')

result = cursor.fetchall()
print(result)

cursor.execute('DROP TABLE test_table')

conn.close()
```