
# 正则表达式

见第13章 正则表达式部分.

```python
import re  
  
import requests  
  
# 定义请求头，模拟Chrome浏览器  
headers = {  
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/537.36',  
}  
  
# 发起GET请求  
url = 'https://www.bookstack.cn'  
response = requests.get(url, headers=headers)  
  
# 检查响应状态码  
if response.status_code == 200:  
    # 打印响应内容  
    # print(response.text)  
    x = re.compile(r'<title>.*?</title>',re.S)  
    print(x.findall(response.text))  
else:  
    print(f"请求失败，状态码：{response.status_code}")
```


# `XPath`

`XPath`是一个强大的XML和HTML解析库，支持`XPath`查询。

## 安装

```Bash
pip install lxml
```

## 基础示例

下面代码先从`lxml.html`模块导入`html`，然后使用`requests`库获取网页内容。之后，它使用`html.fromstring()`方法将HTML字符串转换为一个可以执行XPath查询的对象。最后，使用XPath表达式来选择和提取所需的信息。

```python
from lxml import html
import requests

# 定义请求头，模拟Chrome浏览器
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/537.36',
}

# 发起GET请求
url = 'https://www.bookstack.cn'
response = requests.get(url, headers=headers)

# 检查响应状态码
if response.status_code == 200:
    # 解析HTML
    tree = html.fromstring(response.text)

    # 使用XPath查询
    titles = tree.xpath('//title/text()')
    print(titles)

    # 查询所有带有href属性的a标签
    links = tree.xpath('//a/@href')
    print(links)

    # 查询所有带有特定class的div标签
    divs_with_class = tree.xpath('//div[@class="manual-body cate-list"]')
    print(divs_with_class)

    # 查询所有带有特定title属性的a标签，并获取其href和title属性
    books_info = tree.xpath('//a[@title]/@href | //a[@title]/@title')
    print(books_info)

else:
    print(f"请求失败，状态码：{response.status_code}")

```

## 常用方法

### 选择节点

`/`：从根节点开始选择。
`//`：从当前节点的任意后代节点中选择。
`.`：选择当前节点。
`..`：选择当前节点的父节点。
`@`：选择属性。

```Xpath
//div[@class='example']  // 选择所有class为'example'的div元素
```

### 选择节点的子节点

`child`:: 或省略此部分直接跟元素名。

```Xpath
//div[@class='example']/p  // 选择所有class为'example'的div元素下的p元素
```

### 选择节点的属性

`@attributeName`

```Xpath
//img/@src  // 选择所有img元素的src属性
```

### 选择节点的文本内容

`text()`

```Xpath
//p/text()  // 选择所有p元素的文本内容
```

### 选择节点的父节点

`parent::`

```Xpath
//p/parent::*  // 选择所有p元素的父节点
```

### 选择节点的兄弟节点

`following-sibling::`
`preceding-sibling::`

```Xpath
//p/following-sibling::div  // 选择所有p元素之后的div兄弟元素
```

### 条件选择

`[condition]`：选择满足条件的节点。

```Xpath
//div[@class='example'][position() < 3]  // 选择前两个class为'example'的div元素
```

### 使用函数

`count()`：计算节点的数量。
`string()`：将节点转换为字符串。
`normalize-space()`：移除多余的空白字符。

```Xpath
count(//div[@class='example'])  // 计算class为'example'的div元素的数量
```

 ### 组合选择器
`,`：选择多个节点集合。
`|`：选择多个节点集合的并集

```Xpath
//div[@class='example'] | //div[@class='sample']  // 选择class为'example'或'sample'的所有div元素
```

### 命名空间处理

如果文档中有命名空间，需要使用`namespace-uri()`函数或在XPath表达式中声明命名空间前缀。

```Xpath
//*[local-name()='elementName' and namespace-uri()='http://www.example.com/ns']
```


# BeautifulSoup4，lxml

BeautifulSoup4 是 Python 中一个用于解析 HTML 和 XML 文档的库。可以以一种简单直观的方式浏览、搜索和修改文档树。`BeautifulSoup4` 支持多种解析器，包括 Python 标准库中的 `html.parser`、`lxml` 和 `html5lib`
## 安装

```Bash
pip install beautifulsoup4
```

如果需要使用 `lxml` 解析器，也需要安装 `lxml`：

```Bash
pip install lxml
```

## 基本使用

```python
from bs4 import BeautifulSoup

html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title"><b>The Dormouse's story</b></p>

<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>

<p class="story">...</p>
"""

soup = BeautifulSoup(html_doc, 'html.parser')

# 打印整个文档
print(soup.prettify())

# 获取标题
title = soup.title.string
print(title)

# 获取第一个段落
first_paragraph = soup.p
print(first_paragraph)

# 获取所有的链接
links = soup.find_all('a')
for link in links:
    print(link.get('href'))
```

## 查找元素

BeautifulSoup 提供了多种方法来查找元素，如 `find`, `find_all`, `select` 等。

```python
# 查找第一个段落
first_p = soup.find('p')
print(first_p)

# 查找所有段落
all_ps = soup.find_all('p')
for p in all_ps:
    print(p.text)

# 查找所有带有 class "sister" 的 a 标签
sisters = soup.find_all('a', class_='sister')
for sister in sisters:
    print(sister['href'])
```

## CSS 选择器 select

```python
# 使用 CSS 选择器查找所有 a 标签
a_tags = soup.select('a')
for a in a_tags:
    print(a.text)

# 查找 id 为 link2 的 a 标签
link2 = soup.select('#link2')[0]
print(link2['href'])
```

## 修改文档树

`BeautifulSoup` 允许你修改文档树：

```python
# 添加一个新的 a 标签
new_link = soup.new_tag("a", href="http://example.com/new")
new_link.string = "New Link"
soup.body.append(new_link)
print(soup.prettify())
```



# Selenium

Selenium 是用于自动化 Web 浏览器的强大工具。使用 Python 和 Selenium，可以自动执行表单提交、屏幕抓取和测试 Web 应用程序等任务。Selenium 本质上是一套Web自动化测试工具,在爬虫时一般用于需要强页面交互情况下如:验证码等情况

## 安装

```python
pip install selenium
```


## 下载 WebDriver  

对于 Chrome，下载并安装 [ChromeDriver](https://sites.google.com/a/chromium.org/chromedriver/downloads) .


## 基本操作

### 打开网站

```python
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By

service = Service('/path/to/chromedriver')
driver = webdriver.Chrome(service=service)

driver.get('http://www.example.com')

# Find an element by ID
element = driver.find_element(By.ID, 'myElement')
element.click()

# Close the browser
driver.quit()
```

### 处理元素

```python
# Fill out a form
input_field = driver.find_element(By.NAME, 'username')
input_field.send_keys('myUsername')

# Click a button
button = driver.find_element(By.XPATH, '//button[@id="submit"]')
button.click()
```

### 等待元素 

使用显式等待来确保元素在与它们交互之前已准备就绪

```python
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

wait = WebDriverWait(driver, 10)
element = wait.until(EC.element_to_be_clickable((By.ID, 'myButton')))
element.click()
```

### 处理多个窗口  

在多个浏览器窗口之间导航

```python
# Open a new window
driver.execute_script("window.open('');")
driver.switch_to.window(driver.window_handles[-1])
driver.get('http://www.another-example.com')

# Switch back to the original window
driver.switch_to.window(driver.window_handles[0])
```

### 屏幕截图和日志记录  

捕获屏幕截图并记录错误

```python
# Take a screenshot
driver.save_screenshot('screenshot.png')

# Log messages
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
logger.info('Test passed.')
```

### 处理警告和弹窗

使用 `switch_to.alert` 方法来处理警告和弹出窗口

```python
alert = driver.switch_to.alert
alert.accept()  # 接受警告
# 或者
alert.dismiss()  # 取消警告
alert.send_keys('text')  # 向警告发送文本
```

### 操作Cookies

```python
# 添加一个Cookie
driver.add_cookie({'name': 'myCookie', 'value': 'myValue'})

# 获取所有Cookies
cookies = driver.get_cookies()

# 删除特定的Cookie
driver.delete_cookie('myCookie')

# 删除所有Cookies
driver.delete_all_cookies()
```

### 使用`<select>`元素

```python
from selenium.webdriver.support.ui import Select

dropdown = Select(driver.find_element(By.ID, 'myDropdown'))
dropdown.select_by_visible_text('选项文本')
dropdown.select_by_value('optionValue')
dropdown.select_by_index(1)
```

### 无头模式运行

不开启可视化的浏览器窗口运行测试

```python
options = webdriver.ChromeOptions()
options.add_argument('--headless')
driver = webdriver.Chrome(options=options)
```

### 并行

使用`Selenium Grid`在多台机器上执行测试

```python
from selenium import webdriver
from selenium.webdriver.remote.command import Command

capabilities = {'browserName': 'chrome'}
driver = webdriver.Remote(
   command_executor='http://127.0.0.1:4444/wd/hub',
   desired_capabilities=capabilities
)
```

### Page Object模式

Page Object模式是一种设计模式，用于封装Web页面的元素和行为。这有助于保持代码的清晰和维护性

```python
class LoginPage:
    def __init__(self, driver):
        self.driver = driver
        self.username_input_id = 'username'
        self.password_input_id = 'password'
        self.login_button_id = 'login'

    def login(self, username, password):
        self.driver.find_element(By.ID, self.username_input_id).send_keys(username)
        self.driver.find_element(By.ID, self.password_input_id).send_keys(password)
        self.driver.find_element(By.ID, self.login_button_id).click()
```


### 示例(滑动验证码)

```Bash
pip install selenium opencv-python-headless pillow numpy
```

```python
import cv2
import numpy as np
from PIL import Image
from io import BytesIO
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains

# 设置ChromeDriver的路径
service = Service('/path/to/chromedriver')
options = webdriver.ChromeOptions()
driver = webdriver.Chrome(service=service, options=options)

# 访问目标网站
driver.get('http://example.com/login')

# 等待滑动验证码出现
wait = WebDriverWait(driver, 10)
captcha = wait.until(EC.presence_of_element_located((By.ID, 'captcha')))

# 截取滑动验证码的图片
captcha_screenshot = captcha.screenshot_as_png
image = Image.open(BytesIO(captcha_screenshot))

# 将图片转换为OpenCV格式
image_np = np.array(image)
image_gray = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)

# 图像处理，识别缺口位置
# 这里省略了具体的图像处理细节，需要根据实际情况调整
# 假设缺口位置已经被识别出来，存储在变量gap_x中

gap_x = 123  # 假设的缺口位置

# 模拟滑动操作
slider = driver.find_element(By.ID, 'slider')
action_chains = ActionChains(driver)
action_chains.click_and_hold(slider).perform()
action_chains.move_by_offset(gap_x + 10, 0).perform()  # 额外加上10是为了模拟人类的误差
action_chains.release().perform()

# 验证是否成功
# 在这里添加逻辑判断是否成功通过验证，例如检查页面是否跳转或查看是否有错误提示

# 关闭浏览器
driver.quit()

```

#  Scrapy

`Scrapy` 是一个用于 Python 的快速高级 Web 爬虫框架，用于抓取网站并从页面中提取结构化的数据。它可以用于各种用途，包括数据挖掘、信息处理或存储历史数据。`Scrapy` 的设计目标之一是为了处理大规模的爬虫项目，因此它提供了强大的功能和灵活性。

## Scrapy 的主要特点：

- **异步处理**：`Scrapy` 使用 `Twisted` 异步网络框架，能够同时处理多个请求，提高爬虫效率。
- **可扩展性**：`Scrapy` 设计灵活，易于扩展，可以添加中间件和管道来处理数据。
- **内置数据处理**：提供了 `Item Pipeline` 来处理爬取的数据，如清洗、验证、存储等。
- **自动遵从网站robots.txt规则**：Scrapy 自动遵守网站的 robots.txt 文件，避免不必要的法律问题。
- **动态加载**：支持 AJAX 动态加载的页面，虽然默认不支持 JavaScript 渲染，但可以结合其他工具如 `Selenium` 使用。
- **详细的统计信息**：`Scrapy` 提供详细的爬虫运行统计信息，便于监控和调试。

## 初始化项目

```Bash
   scrapy startproject myproject
```

## 定义 `Item`

在` items.py `文件中定义你要抓取的数据结构。

```python
   import scrapy

   class MyprojectItem(scrapy.Item):
       title = scrapy.Field()
       link = scrapy.Field()
       desc = scrapy.Field()
```

## 编写 Spider

`Spider` 是 `Scrapy` 中负责抓取数据的核心组件。在 `spiders` 目录下创建一个 `Spider` 文件。

```python
   import scrapy
   from myproject.items import MyprojectItem

   class ExampleSpider(scrapy.Spider):
       name = 'example'
       allowed_domains = ['example.com']
       start_urls = ['http://example.com']

       def parse(self, response):
           for sel in response.xpath('//div[@class="item"]'):
               item = MyprojectItem()
               item['title'] = sel.xpath('h2/text()').get()
               item['link'] = sel.xpath('a/@href').get()
               item['desc'] = sel.xpath('p/text()').get()
               yield item
```

## 设置中间件和管道

在 `settings.py` 文件中配置中间件和管道，用于处理请求和响应，以及处理提取的数据。

## 运行 `Spider`

通过命令行运行 `Spider`。

```Bash
   scrapy crawl example
```

